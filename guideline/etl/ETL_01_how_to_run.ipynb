{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL how to run?\n",
    "> At here we will talk about how to run ETL. There is 2 steps to run ETL.\n",
    "\n",
    "1. prepare config\n",
    "2. put config to ETLPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. prepare config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "# E = Extract, T = Transform, L = Load\n",
    "E_path = \"/data/private/dataverse/dataverse/config/etl/sample/data_ingestion___sampling.yaml\"\n",
    "T_path = \"/data/private/dataverse/dataverse/config/etl/sample/data_preprocess___dedup.yaml\"\n",
    "L_path = \"/data/private/dataverse/dataverse/config/etl/sample/data_load___hf_obj.yaml\"\n",
    "\n",
    "E_config = OmegaConf.load(E_path)\n",
    "T_config = OmegaConf.load(T_path)\n",
    "L_config = OmegaConf.load(L_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Config\n",
    "\n",
    "- load huggingface dataset `RedPajama-Data-1T-Sample`\n",
    "- Convert huggingface dataset to UFL format\n",
    "    - `ufl format` is the following structure `List[Dict]`\n",
    "- sample 1% of total data to reduce the size of dataset\n",
    "- save to parquet `./sample/pajama_sample_ufl.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark:\n",
      "  appname: dataverse_etl_sample\n",
      "  driver:\n",
      "    memory: 16g\n",
      "etl:\n",
      "- name: data_ingestion___red_pajama___hf2ufl\n",
      "- name: utils___sampling___random\n",
      "  args:\n",
      "    sample_n_or_frac: 0.01\n",
      "- name: data_load___parquet___ufl2parquet\n",
      "  args:\n",
      "    save_path: ./sample/pajama_sample_ufl.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(E_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Config\n",
    "\n",
    "- load parquet `./sample/pajama_sample_ufl.parquet`\n",
    "- deduplicate by `text` column, 15-gram minhash jaccard similarity\n",
    "- save to parquet `./sample/pajama_preprocess_ufl.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark:\n",
      "  appname: dataverse_etl_sample\n",
      "  driver:\n",
      "    memory: 16g\n",
      "etl:\n",
      "- name: data_ingestion___ufl___parquet2ufl\n",
      "  args:\n",
      "    input_paths:\n",
      "    - ./sample/pajama_sample_ufl.parquet\n",
      "- name: deduplication___polyglot___minhash\n",
      "- name: data_load___parquet___ufl2parquet\n",
      "  args:\n",
      "    save_path: ./sample/pajama_preprocess_ufl.parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(T_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Config\n",
    "\n",
    "- load parquet `./sample/pajama_preprocess_ufl.parquet`\n",
    "- convert to huggingface dataset and return the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark:\n",
      "  appname: dataverse_etl_sample\n",
      "  driver:\n",
      "    memory: 16g\n",
      "etl:\n",
      "- name: data_ingestion___ufl___parquet2ufl\n",
      "  args:\n",
      "    input_paths:\n",
      "    - ./sample/pajama_preprocess_ufl.parquet\n",
      "- name: data_load___huggingface___ufl2hf_obj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(L_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. put config to ETLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataverse.etl import ETLPipeline\n",
    "\n",
    "etl_pipeline = ETLPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/22 02:31:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found cached dataset red_pajama-data-1_t-sample (/root/.cache/huggingface/datasets/togethercomputer___red_pajama-data-1_t-sample/plain_text/1.0.0/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336518740b4d4d1886eeebc1f0cb68bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /root/.cache/dataverse/dataset/huggingface_0213b3219ff7d503.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset spark/2077475119 to /root/.cache/huggingface/datasets/spark/2077475119/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset spark downloaded and prepared to /root/.cache/huggingface/datasets/spark/2077475119/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# raw -> ufl\n",
    "etl_pipeline.run(E_config)\n",
    "\n",
    "# ufl -> dedup -> ufl\n",
    "etl_pipeline.run(T_config)\n",
    "\n",
    "# ufl -> hf_obj\n",
    "dataset = etl_pipeline.run(L_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'meta', 'name', 'text'],\n",
       "    num_rows: 9150\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'c126e4e658a411eeb745d652e8cce297',\n",
       " 'meta': \"{'timestamp': '2019-04-23T03:54:30Z', 'url': 'https://www.globaltuners.com/forum/thread/1924?page=1', 'language': 'en', 'source': 'c4'}\",\n",
       " 'name': 'red_pajama',\n",
       " 'text': \"We've been working on a completely new mobile app for GlobalTuners and we have just released the first Android version for public beta testing. A version for iOS will hopefully follow later.\\nAt the moment the app provides just the basic functionality to listen to and control the receivers, and we would like your feedback. Do you think it's missing something, something is confusing or just broken?\\nA Premium Membership is needed to be able to use the new app. If you do not have a premium subscription and are not sharing a receiver, you can get a premium subscription via the app and your Google Play account at a discounted price (about 4 euros per month).\\nIf you're interested in testing the new app, you can opt in for the beta test at https://play.google.com/apps/testing/com.globaltuners.android.v1 and install it via the Google Play Store.\\nBTW: can someone close to the Nederlands record GrootNeus Radio on 1008KHz and RTBF Int on 621KHz and 1125 KHz when they close?\\nWill it have the functionality to show people in each radio that are listening ? I found that to be problematic. Many times I would go in on the old app and it showed no one was tuning while on the laptop version there clearly was someone tuning and I hijacked unknowingly the radio .\\nseems good but found when I tried to tune 104.3 WFM it came in as 104...is this any enter key? Is bandguide going to be added as well ? Thanks, Ivo.\\nI'd love to try this, though my Google play account (address removed by admin) is different from the mail one I use for Globaltuners and sharing receivers. I still have the old GT app but hopefully this one works with HD.\\n@robertr64: that's not a problem, you can use the gmail.com address to join the beta and install the app and then sign in with your GT account in the app.\\nThe old app should still work, but will stop working eventually since it has not been updated for a long time and continued support for the old app would make the rest of GlobalTuners less secure. The new app responds much faster and should support all modes and options on the receivers, including HD channels.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[777]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
